---
sidebar_position: 1
authors:
  - name: Daniel Bazo Correa
title: Tensorflow
toc_max_heading_level: 3
---

<div align="justify">

# Tensorflow

## Bibliograf√≠a

- [https://www.tensorflow.org/api_docs](https://www.tensorflow.org/api_docs)
- [Deep Learning by deeplearning.ai | Coursera](https://www.coursera.org/specializations/deep-learning)
- [Certificado profesional de DeepLearning.AI desarrollador de TensorFlow | Coursera](https://www.coursera.org/professional-certificates/tensorflow-in-practice)

## Anexo A: Terminolog√≠a √∫til

Aqu√≠ se re√∫nen las palabras, t√©rminos, conceptos etc. claves a tener en cuenta a modo de
recordatorio. No tiene ning√∫n tipo de orden, ni pretende ser el primer cap√≠tulo a
visualizar.

- **La letra ‚Äòx‚Äô se asocia a la variable independiente**, lo que usamos para hacer
  predicciones, por ejemplo im√°genes. Mientras que la **letra ‚Äòy‚Äô se asocia a la variable
  dependiente**, lo que se denominan etiquetas y es nuestro objetivo obtener una
  predicci√≥n que tenga una alta probabilidad de parecerse a dicha ‚Äòy‚Äô, un ejemplo de
  etiquetas pueden ser los nombres de las im√°genes que permiten clasificar razas de
  perros.
- **weigth** = pesos: valores aleatorios con los que se inicializan a las neuronas, estos
  par√°metros son fundamentales para determinar el tipo de funcionamiento de una red.
  - Forma (shape) ‚Üí w(tama√±o input, n√∫mero de neuronas)
- **bias** = sesgo:
  - Forma (shape) ‚Üí b(1, n√∫mero de neuronas)
- Un **set de datos de puede dividir** en:
  - Set de entrenamiento (train set).
  - Set de desarrollo o validaci√≥n (dev set o validation set).
  - Set de pruebas (test set)
- Funci√≥n de p√©rdida (**Loss function**): funci√≥n que mide el rendimiento del modelo en
  uno de los ejemplos del set de entrenamiento.
- Funci√≥n de coste (**Cost function**): funci√≥n que mide el rendimiento del modelo en
  todos los ejemplos del set de entrenamiento. Ser√≠a la media de cada una de las
  funciones de p√©rdidas de cada ejemplo del set de entrenamiento.
- **Dataset**: una colecci√≥n que devuelve un tuple de su variable independiente y
  dependiente para un solo elemento.
- **DataLoader**: un iterador que proporciona un flujo de grupos reducidos
  (mini-batches), donde cada grupo reducido es un tuple de un lote de variables
  independientes y un lote de variables dependientes.
- **one-hot encoding**: vectores de 0‚Äôs con el tama√±o del n√∫mero de clases que tenga el
  dataset, cada categor√≠a representa una posici√≥n en el vector por lo que si la imagen
  contiene alg√∫n elemento de alguna clase el vector tendr√° un 1 en la posici√≥n del vector
  que corresponder√° a su clase.
- Es importante saber que **una clasificaci√≥n** pretende predecir una clase o categor√≠a
  mientras que un modelo de **regresi√≥n** intenta predecir 1 o m√°s cantidades num√©ricas.

## Cap√≠tulo 0: √ötiles

### 0.1. Descomprimir ficheros

Muchos de los datasets se encuentran comprimidos, si usamos Colab ser√° pesado subir los
ficheros a Drive adem√°s del tiempo requerido para ello.

```python
import zipfile
import os

arhicvo_zip_descomprimir = './cats_and_dogs_filtered.zip'
lugar_descomprimir = ''

if os.path.exists(lugar_descomprimir) == False:

	zip_ref = zipfile.ZipFile(lugar_descomprimir , 'r')
	zip_ref.extractall()
	zip_ref.close()
```

### 0.2. Utilizar Weights & Biases (wandb)

Podemos utilizar WandB para el registro de la evoluci√≥n de nuestro m√≥dulo. Utilizar WandB
nos permite obtener un registro del aprendizaje del modelo, compartir informaci√≥n en
tiempo real, etc.

```python
import wandb

!wandb login

# Ejemplo de par√°metros que podemos rellenar para crear una instancia
wandb.init(
    settings = wandb.Settings(start_method="thread"),
    project = "Transfer-Learning-Inception-Perros-vs-Gatos",
    config = {
        "learning_rate": lr,
        "epochs": num_epocas,
        "batch_size": tam_batch,
        "loss_function": "binary_crossentropy",
        "architecture": "Inception",
        "dataset": "Dog-vs-Cat"
        }
    )

config = wandb.config
```

### 0.3. Utilizar modelos pre-entrenados

Al importar la librer√≠a de `Tensorflow`, podemos utilizar `tf.keras.applications.` para
que aparezca la lista de modelos pre-entrenados disponibles.

Tener en cuenta, que dependiendo del modelo pre-entrenado a utilizar deberemos seguir
pasos espec√≠ficos. Por ejemplo, por c√≥mo ha sido entrenado ResNet no podemos utilizar ni
`BatchNormalization()` ni tampoco realizar normalizaci√≥n en los datos ya que requiere
realizar el procesado de la siguiente manera:

```python
train_datagen = ImageDataGenerator(
    dtype='float32',
    preprocessing_function = preprocess_input,
    horizontal_flip = True,
    validation_split = 0.05 # Division del dataset de entrenamiento a 80/20, 20 para validaci√≥n
)
```

El ejemplo anterior es uno de los m√∫ltiples casos que se puedan dar por lo que lo m√°s
recomendado es visitar siempre la documentaci√≥n de `Tensorflow` .

Lo primero para poder hacer uso de los modelos pre-entrenados de `Tensorflow` es cargar
el modelo con los ajustes deseados:

```python
import tensorflow as tf

#   Modelo
modelo_base = tf.keras.applications.ResNet50(
  weights = 'imagenet', # Pesos con los que ha sido entrenado el modelo
  input_shape = (224, 224, 3), # Dimensi√≥n de los datos esperada por el modelo
  include_top = False # Eliminamos las √∫ltimas capas
)
```

Posteriormente, debemos congelar los par√°metros del modelo para no reentrenarlos.

```python
for capa in modelo_base.layers:

  capa.trainable = False
```

Para ajustar el modelo pre-entrenado a nuestro problema, debemos a√±adir nuevas capas al
modelo. Estas ser√°n las capas que entrenaremos en primera instancia para posteriormente
descongelar el modelo y entrenarlo al completo.

```python
def Modelo(modelo_base):

		# Con modelo_base.summary() podemos ver todas las capas que cuenta el modelo
		# Estas capas se encuentran asociadas a un nombre que tambi√©n se ve reflejado
		# al usar el m√©todo summary() por lo que apuntamos el nombre de la √∫ltima capa
    ultima_capa = modelo_base.get_layer('conv5_block3_out')
    salida_capa = ultima_capa.output

    x = layers.Flatten()(salida_capa)

    x = layers.Dense(units = 512, activation = 'relu')(x)
    x = layers.Dropout(rate = 0.2)(x)

    x = layers.Dense(units = 256, activation = 'relu')(x)
    x = layers.Dropout(rate = 0.2)(x)

    x = layers.Dense(units = num_clases, activation = 'softmax')(x)

    modelo = Model(modelo_base.input, x)

    modelo.compile(
        optimizer = Optimizer.Adam(learning_rate = lr),
        loss = loss.CategoricalCrossentropy(),
        metrics = ['accuracy']
    )

    return modelo

modelo = Modelo(modelo_base)
```

### 0.4. Mostrar gr√°ficas de p√©rdidas y accuracy

```python
history = modelo.fit(...)

# Creamos una gr√°fica para mostrar el accuracy obtenido tanto en el set de entrenamiento como en el de validacion
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.grid(which = 'both', axis = 'both')
plt.title('Precisi√≥n del modelo')
plt.ylabel('Precisi√≥n')
plt.xlabel('√âpoca')
plt.legend(['Entrenamiento', 'Test'], loc='upper right')
plt.show()

# Creamos una gr√°fica para mostrar la funci√≥n de p√©rdida obtenida tanto en el set de entrenamiento como en el de validacion
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.grid(which = 'both', axis = 'both')
plt.title('P√©rdida del modelo')
plt.ylabel('P√©rdida')
plt.xlabel('√âpoca')
plt.legend(['Entrenamiento', 'Test'], loc='upper right')
plt.show()
```

### 0.5. ImageDataGenerator

Podemos utilizar generadores de datos para la manipulaci√≥n de datos de nuestro dataset.
Con ello, conseguimos realizar aumentaci√≥n datos, divisiones de datos para entrenamiento,
validaci√≥n y pruebas, entre otras herramientras.

Un posible uso ser√≠a el siguiente:

```python
train_datagen = ImageDataGenerator(
    dtype='float32',
    preprocessing_function = preprocess_input,
    horizontal_flip = True,
    validation_split = 0.05 # 5% de los datros de entrenamiento para validaci√≥n
)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    batch_size = num_batch,
    class_mode = "categorical",
    target_size = (224, 224),
    shuffle = True,
    subset = 'training'
    )

validation_generator =  train_datagen.flow_from_directory(
    train_dir,
    batch_size = num_batch,
    class_mode  = "categorical",
    target_size = (224, 224),
    shuffle = True,
    subset = 'validation'
    )
```

A la hora de utilizar generadores de datos tenemos que ajustar los par√°metros de la
funci√≥n .fit:

```python
history = modelo.fit(
            train_generator,
            steps_per_epoch = train_generator.samples // num_batch,
            epochs = 50,
            callbacks = callbacks,
            verbose = 1,
            validation_data = validation_generator,
            validation_steps = validation_generator.samples // num_batch
            )
```

## Cap√≠tulo 1: Visi√≥n Computacional

### 1.0. Teor√≠a previa

#### 1.0.1. Redes Neuronales Convolucionales (CNN)

#### 1.0.2. Transfer Learning

#### 1.0.3. Tipos de arquitecturas para visi√≥n computacional

### 1.1. Lenguaje de signos, SIGN MNIST

#### 1.1.1. Anotaciones

#### 1.1.2. C√≥digo

```python
import tensorflow.keras as keras
import pandas as pd
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

# Cargamos los datos desde los ficheros CSV
train_df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/data/sign_mnist_train.csv")
valid_df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/data/sign_mnist_valid.csv")

# Separamos el contenido de los ficheros CSV
y_train = train_df['label']
y_valid = valid_df['label']
del train_df['label']
del valid_df['label']

# Obtenemos los datos de entrenamiento y validaci√≥n
x_train = train_df.values
x_valid = valid_df.values

# Obtenemos nuestras etiquetas utilizando categorias binarias (one-hot)
num_classes = 24
y_train = keras.utils.to_categorical(y_train, num_classes)
y_valid = keras.utils.to_categorical(y_valid, num_classes)

x_train = x_train.reshape(-1,28,28,1)
x_valid = x_valid.reshape(-1,28,28,1)
print(x_train.shape)
print(x_valid.shape)

def Modelo():

    modelo = tf.keras.models.Sequential([
        tf.keras.layers.Conv2D(32, (3, 3), strides=1, padding="same", activation="relu", input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPool2D((2, 2), strides=2, padding="same"),
        tf.keras.layers.BatchNormalization(),

        tf.keras.layers.Conv2D(64, (3, 3), strides=1, padding="same", activation="relu"),
        tf.keras.layers.MaxPool2D((2, 2), strides=2, padding="same"),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.25),

        tf.keras.layers.Conv2D(64, (3, 3), strides=1, padding="same", activation="relu"),
        tf.keras.layers.MaxPool2D((2, 2), strides=2, padding="same"),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.25),

        tf.keras.layers.Conv2D(128, (3, 3), strides=1, padding="same", activation="relu"),
        tf.keras.layers.MaxPool2D((2, 2), strides=2, padding="same"),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.25),

         tf.keras.layers.Flatten(),

        tf.keras.layers.Dense(units=512, activation="relu"),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.5),

        tf.keras.layers.Dense(units=256, activation="relu"),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.5),

        tf.keras.layers.Dense(units=num_classes, activation="softmax")
    ])

    modelo.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-3),
        loss=tf.keras.losses.CategoricalCrossentropy(),
        metrics=['accuracy']
        )

    return modelo

model = Modelo()

train_datagen = ImageDataGenerator(
    rescale = 1./255,
    rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)
    zoom_range=0.1,  # Randomly zoom image
    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
    horizontal_flip=True,  # randomly flip images horizontally
    brightness_range = (0.4, 0.75),
    validation_split = 0.2
)

test_datagen = ImageDataGenerator(
    rescale = 1./255
)

train_gen = train_datagen.flow(
    x_train,
    y_train,
    batch_size = 64,
    subset = 'training'
)

valid_gen = train_datagen.flow(
    x_train,
    y_train,
    batch_size = 64,
    subset = 'validation'
)

test_gen = test_datagen.flow(
    x_valid,
    y_valid,
    batch_size=64,
)

class CallBack(tf.keras.callbacks.Callback):

    def on_epoch_end(self, epocas, logs = {}):

        if(logs.get('val_accuracy') >= 0.98):

            print("\nSe alcanzo un 98% de precision, cancelamos el entrenamiento")
            self.model.stop_training = True

callback = CallBack()

# Para la reducci√≥n del learning rate atenderemos a la m√©trica de la precisi√≥n del set de validaci√≥n
reducir_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor = 'val_accuracy', factor = 0.1,
    patience = 3, min_lr = 1e-5, verbose = 1
)

callbacks = [callback, reducir_lr]

history = model.fit(train_gen,
          epochs = 30,
          validation_data = valid_gen,
          callbacks = callbacks
          )

# Creamos una gr√°fica para mostrar el accuracy obtenido tanto en el set de entrenamiento como en el de validacion
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.grid(which = 'both', axis = 'both')
plt.title('Precisi√≥n del modelo')
plt.ylabel('Precisi√≥n')
plt.xlabel('√âpoca')
plt.legend(['Entrenamiento', 'Test'], loc='upper right')
plt.show()

# Creamos una gr√°fica para mostrar la funci√≥n de p√©rdida obtenida tanto en el set de entrenamiento como en el de validacion
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.grid(which = 'both', axis = 'both')
plt.title('P√©rdida del modelo')
plt.ylabel('P√©rdida')
plt.xlabel('√âpoca')
plt.legend(['Entrenamiento', 'Test'], loc='upper right')
plt.show()
```

### 1.2. Perros vs Gatos

#### 1.2.1. Anotaciones

#### 1.2.2. C√≥digo

```python
import os
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import zipfile

!wget --no-check-certificate https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip

#   Hiperpar√°metros
num_epocas = 40
lr = 1e-3
batch_size = 64

local_zip = './cats_and_dogs_filtered.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall()
zip_ref.close()

#   Vamos a cargar el dataset
#   El dataset se encuentra en E:\Datasets\data\dogs-vs-cats, contamos con 2 carpetas: train y test
base_dir = './cats_and_dogs_filtered'

print("Contenido en el directorio base:")
print(os.listdir(base_dir))

print("\nContenido en el directorio de entrenamiento:")
# Con mostrar algunos ejemplos es suficiente
print(os.listdir(f'{base_dir}/train'))

print("\nContenido en el directorio de test:")
print(os.listdir(f'{base_dir}/validation'))

train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'validation')

#   Modelo
def Modelo():

    modelo = tf.keras.models.Sequential([

        tf.keras.layers.Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), input_shape = (150, 150, 3), activation = 'relu'),
        tf.keras.layers.MaxPool2D(pool_size = (2, 2)),
        tf.keras.layers.BatchNormalization(),
        #tf.keras.layers.Dropout(rate = 0.25),

        tf.keras.layers.Conv2D(filters = 64, kernel_size = (3, 3), strides = (1, 1), activation = 'relu'),
        tf.keras.layers.MaxPool2D(pool_size = (2, 2)),
        tf.keras.layers.BatchNormalization(),
        #tf.keras.layers.Dropout(rate = 0.25),

        tf.keras.layers.Conv2D(filters = 64, kernel_size = (3, 3), strides = (1, 1), activation = 'relu'),
        tf.keras.layers.MaxPool2D(pool_size = (2, 2)),
        tf.keras.layers.BatchNormalization(),
        #tf.keras.layers.Dropout(rate = 0.25),

        tf.keras.layers.Conv2D(filters = 128, kernel_size = (3, 3), strides = (1, 1), activation = 'relu'),
        tf.keras.layers.MaxPool2D(pool_size = (2, 2)),
        tf.keras.layers.BatchNormalization(),
        #tf.keras.layers.Dropout(rate = 0.25),

        tf.keras.layers.Flatten(),

        tf.keras.layers.Dense(units = 512, activation = 'relu'),
        #tf.keras.layers.Dropout(rate = 0.5),
        tf.keras.layers.Dense(units = 128, activation = 'relu'),
        #tf.keras.layers.Dropout(rate = 0.5),
        tf.keras.layers.Dense(units = 1, activation = 'sigmoid')
    ])

    modelo.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate = lr),
        loss=tf.keras.losses.BinaryCrossentropy(),
        metrics=['accuracy']
        )

    return modelo

train_datagen = ImageDataGenerator(
    rescale = 1./255,
    vertical_flip = True,
    horizontal_flip = True,
    rotation_range = 40, # Gira entre 0 y 40 grados
    width_shift_range = 0.2, # Cantidad aleatoria para desplazar el centro de una imagen
    height_shift_range = 0.2,
    shear_range = 0.2, # shear = cizallar, realizar cortes en la imagen para cambiar perspectivas por ejemplo
    zoom_range = 0.2,
    fill_mode = 'nearest' # Intenta recrear la informaci√≥n perdida tras una transformaci√≥n como shear
    )

train_datagen = ImageDataGenerator(rescale = 1.0/255)

test_datagen  = ImageDataGenerator(rescale = 1.0/255)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    batch_size = batch_size,
    class_mode = 'binary',
    target_size = (150, 150)
    )

validation_generator =  test_datagen.flow_from_directory(
    validation_dir,
    batch_size = batch_size,
    class_mode  = 'binary',
    target_size = (150, 150)
    )

modelo = Modelo()
modelo.summary()

early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor = 'val_loss',
    min_delta = 1e-3,
    patience = 10,
    verbose = 1,
    mode = 'auto',
    restore_best_weights = True
)

# Para la reducci√≥n del learning rate atenderemos a la m√©trica de la precisi√≥n del set de validaci√≥n
reducir_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor = 'val_accuracy', factor = 0.1,
    patience = 8, min_lr = 1e-5, verbose = 1
)

callbacks = [early_stopping, reducir_lr]

history = modelo.fit(
            train_generator,
            epochs = num_epocas,
            callbacks = callbacks,
            verbose = 2,
            validation_data = validation_generator
            )

# Creamos una gr√°fica para mostrar el accuracy obtenido tanto en el set de entrenamiento como en el de validacion
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.grid(which = 'both', axis = 'both')
plt.title('Precisi√≥n del modelo')
plt.ylabel('Precisi√≥n')
plt.xlabel('√âpoca')
plt.legend(['Entrenamiento', 'Test'], loc='upper right')
plt.show()

# Creamos una gr√°fica para mostrar la funci√≥n de p√©rdida obtenida tanto en el set de entrenamiento como en el de validacion
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.grid(which = 'both', axis = 'both')
plt.title('P√©rdida del modelo')
plt.ylabel('P√©rdida')
plt.xlabel('√âpoca')
plt.legend(['Entrenamiento', 'Test'], loc='upper right')
plt.show()
```

### 1.3. Implementaci√≥n YOLO

Para hacer uso de las utilidades, modelo y dem√°s herramientas, debemos descargar las
herramientas del repositorio.

#### 1.3.0. Teor√≠a

#### 1.3.1. Anotaciones

El modelo YOLO ha sido entrenado con datos de entrada con tama√±os de (m, 608, 608, 3).
Cada salida resultante, consiste en un cuadro delimitador (_bounding boxes_) con la clase
reconocida. A su vez, cada cuadro est√° representado por 6 n√∫meros (ùëùùëê, ùëèùë•, ùëèùë¶, ùëè‚Ñé, ùëèùë§,
ùëê). Para este ejemplo de algoritmo de YOLO, usaremos **yad2k** (YAD2K: Yet Another
Darknet 2 Keras) que cuenta con 80 clases.

Las cajas de anclaje se eligen explorando los datos de entrenamiento para elegir
proporciones razonables de altura y anchura que representen mejor a las diferentes
clases. Para esta tarea, se han elegido 5 cajas de anclaje (para cubrir las 80 clases),
almacenados en el archivo './model_data/yolo_anchors.txt'

La dimensi√≥n para las cajas de anclaje es la pen√∫ltima dimensi√≥n en la codificaci√≥n (ùëö,
ùëõùêª, ùëõùëä, ùëéùëõùëê‚Ñéùëúùëüùë†, ùëêùëôùëéùë†ùëíùë†) .

Por tanto, la arquitectura YOLO consiste en:

- IMAGEN (m, 608, 608, 3) ‚Üí CNN PROFUNDA ‚Üí CODIFICACI√ìN (m, 19, 19, 5, 85).

Dado que estamos utilizando 5 cajas de anclaje, cada una de las 19 x19 celdas codifica
informaci√≥n sobre 5 cajas. Las cajas de anclaje se definen √∫nicamente por su anchura y
altura.

Para simplificar, aplanaremos las dos √∫ltimas dimensiones de la codificaci√≥n de la forma
(19, 19, 5, 85), de modo que la salida de la CNN profunda es (19, 19, 425).

Ahora, para cada caja (de cada celda) se calcular√° un producto y se extraer√° una
probabilidad de que la caja contenga una determinada clase. La puntuaci√≥n (_score_) de
clase, calculado como ùëñ = ùëùùëê √ó ùëêùëñ, consistir√≠a en la probabilidad de que haya un objeto
ùëùùëê por la probabilidad de que el objeto sea de una determinada clase ùëêùëñ .

#### 1.3.2. C√≥digo

Al tratarse de un c√≥digo m√°s complejo, iremos dividi√©ndolo en partes acompa√±ados por una
explicaci√≥n.

Primero importamos las librer√≠as necesarias.

```python
import argparse
import os
import matplotlib.pyplot as plt
from matplotlib.pyplot import imshow
import scipy.io
import scipy.misc
import numpy as np
import pandas as pd
import PIL
from PIL import ImageFont, ImageDraw, Image
import tensorflow as tf
from tensorflow.python.framework.ops import EagerTensor

from tensorflow.keras.models import load_model
from yad2k.models.keras_yolo import yolo_head
from yad2k.utils.utils import draw_boxes, get_colors_for_classes, scale_boxes, read_classes, read_anchors, preprocess_image
```

En primer lugar, vamos a aplicar un filtro para el umbral, lo que implica que eliminar√°
cualquier casilla cuya puntuaci√≥n de clase sea inferior a un umbral elegido. El modelo
arrojar√° un total de 19x19x6x85 n√∫meros, con cada caja descrita por 85 n√∫meros. Es
conveniente reordenar el tensor dimensional (19, 19, 5, 85) o (19, 19, 425) en las
siguientes variables:

- **box_confidence**: tensor de forma (19, 19, 5, 1) que contiene ùëùùëê (probabilidad de
  confianza de que haya alg√∫n objeto) para cada una de las 5 cajas previstas en cada una
  de las 19x19 casillas.
- **boxes**: tensor de forma (19, 19, 5, 4) que contiene el punto medio y las dimensiones
  (ùëèùë•, ùëèùë¶, ùëè‚Ñé, ùëèùë§) para cada una de las 5 cajas en cada celda.
- **box_class_probs**: tensor de forma (19, 19, 5, 80) que contiene las "probabilidades
  de clase" (ùëê1, ùëê2, ..., ùëê80) para cada una de las 80 clases para cada una de las 5
  cajas por celda.

```python
def yolo_filter_boxes(boxes, box_confidence, box_class_probs, threshold = .6):

    # Step 1: Compute box scores
    ##(‚âà 1 line)
    box_scores = box_confidence * box_class_probs

    # Step 2: Find the box_classes using the max box_scores, keep track of the corresponding score
    ##(‚âà 2 lines)
    box_classes = tf.math.argmax(input = box_scores, axis = -1)
    box_class_scores = tf.math.reduce_max(input_tensor = box_scores, axis = -1)

    # Step 3: Create a filtering mask based on "box_class_scores" by using "threshold". The mask should have the
    # same dimension as box_class_scores, and be True for the boxes you want to keep (with probability >= threshold)
    ## (‚âà 1 line)
    filtering_mask = box_class_scores >= threshold

    # Step 4: Apply the mask to box_class_scores, boxes and box_classes
    ## (‚âà 3 lines)
    scores = tf.boolean_mask(tensor = box_class_scores, mask = filtering_mask)
    boxes = tf.boolean_mask(tensor = boxes, mask = filtering_mask)
    classes = tf.boolean_mask(tensor = box_classes, mask = filtering_mask)

    return scores, boxes, classes
```

Implementar IoU (_Intersection over Union_). No es necesario implementarlo pero para que
veamos c√≥mo se har√≠a.

```python
def iou(box1, box2):

    (box1_x1, box1_y1, box1_x2, box1_y2) = box1
    (box2_x1, box2_y1, box2_x2, box2_y2) = box2

    # Calculate the (yi1, xi1, yi2, xi2) coordinates of the intersection of box1 and box2. Calculate its Area.
    ##(‚âà 7 lines)
    xi1 = max(box1_x1, box2_x1)
    yi1 = max(box1_y1, box2_y1)
    xi2 = min(box1_x2, box2_x2)
    yi2 = min(box1_y2, box2_y2)
    inter_width = xi2-xi1
    inter_height = yi2-yi1
    inter_area = max(inter_width, 0) * max(inter_height, 0)

    # Calculate the Union area by using Formula: Union(A,B) = A + B - Inter(A,B)
    ## (‚âà 3 lines)
    box1_area = (box1_x2 - box1_x1) * (box1_y2 - box1_y1)
    box2_area = (box2_x2 - box2_x1) * (box2_y2 - box2_y1)
    union_area = box1_area + box2_area - inter_area

    # compute the IoU
    return inter_area / union_area
```

Implementar _Non-Max Suppression._ `Tensorflow` tiene dos funciones incorporadas que se
utilizan para implementar la supresi√≥n de no-m√°ximos (por lo que no es necesario utilizar
la funci√≥n iou()).

```python
def yolo_non_max_suppression(scores, boxes, classes, max_boxes = 10, iou_threshold = 0.5):

    # tensor to be used in tf.image.non_max_suppression()
    max_boxes_tensor = tf.Variable(max_boxes, dtype='int32')

    # Use tf.image.non_max_suppression() to get the list of indices corresponding to boxes you keep
    nms_indices = tf.image.non_max_suppression(
									boxes = boxes,
									scores = scores,
									max_output_size = max_boxes,
									iou_threshold = iou_threshold)

    # Use tf.gather() to select only nms_indices from scores, boxes and classes
    scores = tf.gather(params = scores, indices = nms_indices)
    boxes = tf.gather(params = boxes, indices = nms_indices)
    classes = tf.gather(params = classes, indices = nms_indices)

    return scores, boxes, classes
```

Convertir las predicciones de la caja YOLO en esquinas de la caja delimitadora.

```python
def yolo_boxes_to_corners(box_xy, box_wh):

    box_mins = box_xy - (box_wh / 2.)
    box_maxes = box_xy + (box_wh / 2.)

    return tf.keras.backend.concatenate([
        box_mins[..., 1:2],  # y_min
        box_mins[..., 0:1],  # x_min
        box_maxes[..., 1:2],  # y_max
        box_maxes[..., 0:1]  # x_max
    ])
```

Convertimos la salida de la codificaci√≥n YOLO (un mont√≥n de cajas) en sus cajas predichas
junto con sus puntuaciones, coordenadas de caja y clases.

```python
def yolo_eval(yolo_outputs, image_shape = (720, 1280), max_boxes=10, score_threshold=.6, iou_threshold=.5):

    ### START CODE HERE
    # Retrieve outputs of the YOLO model (‚âà1 line)
    box_xy, box_wh, box_confidence, box_class_probs = yolo_outputs

    # Convert boxes to be ready for filtering functions (convert boxes box_xy and box_wh to corner coordinates)
    boxes = yolo_boxes_to_corners(box_xy, box_wh)

    # Use one of the functions you've implemented to perform Score-filtering with a threshold of score_threshold (‚âà1 line)
    scores, boxes, classes = yolo_filter_boxes(boxes, box_confidence, box_class_probs, score_threshold)

    # Scale boxes back to original image shape.
    boxes = scale_boxes(boxes, image_shape)

    # Use one of the functions you've implemented to perform Non-max suppression with
    # maximum number of boxes set to max_boxes and a threshold of iou_threshold (‚âà1 line)
    scores, boxes, classes = yolo_non_max_suppression(scores, boxes, classes, max_boxes, iou_threshold)

    return scores, boxes, classes
```

Podemos probar un modelo pre-entrenado de YOLO. Dicho modelo ha sido entrenado en el
dataset de COCO basado en un problema de conducci√≥n aut√≥noma.

```python
class_names = read_classes("model_data/coco_classes.txt")
anchors = read_anchors("model_data/yolo_anchors.txt")
model_image_size = (608, 608)
yolo_model = load_model("model_data/", compile=False)
```

Creamos una funci√≥n para poder ejecutar el gr√°fico para predecir las cajas.

```python
def predict(image_file):
    # Preprocess your image
    image, image_data = preprocess_image("images/" + image_file, model_image_size = (608, 608))

    yolo_model_outputs = yolo_model(image_data)
    yolo_outputs = yolo_head(yolo_model_outputs, anchors, len(class_names))

    out_scores, out_boxes, out_classes = yolo_eval(yolo_outputs, [image.size[1],  image.size[0]], 10, 0.3, 0.5)

    # Print predictions info
    print('Found {} boxes for {}'.format(len(out_boxes), "images/" + image_file))
    # Generate colors for drawing bounding boxes.
    colors = get_colors_for_classes(len(class_names))
    # Draw bounding boxes on the image file
    #draw_boxes2(image, out_scores, out_boxes, out_classes, class_names, colors, image_shape)
    draw_boxes(image, out_boxes, out_classes, class_names, out_scores)
    # Save the predicted bounding box on the image
    image.save(os.path.join("out", image_file), quality=100)
    # Display the results in the notebook
    output_image = Image.open(os.path.join("out", image_file))
    imshow(output_image)

    return out_scores, out_boxes, out_classes
```

Finalmente, probamos el modelo:

```python
out_scores, out_boxes, out_classes = predict("download.jpeg")
```

## Cap√≠tulo 2: Procesamiento del Lenguaje Natural, NLP

### 2.0. Teor√≠a

Modelos secuenciales:

Transformers:

## Cap√≠tulo 3: Procesamiento de Audio

### 3.1. Reconocimiento de audio

#### 3.1.1. Anotaciones

#### 3.1.2. C√≥digo

Podemos utilizar la librer√≠a `librosa` de Python para el procesado de se√±ales de audio.
Para este caso, podemos convertir el sonido en espectrogramas para posteriormente tratar
el problema como un clasificador por visi√≥n computacional.

Ejemplo de c√≥digo para procesar se√±ales de audio a espectrogramas:

```python
import librosa
import numpy as np

# Leemos el fichero
file    = "temp/processed_file.wav"
sig, fs = librosa.core.load(file, sr=8000)

# Procesamos el fichero
abs_spectrogram = np.abs(librosa.core.spectrum.stft(sig))
audio_signal = librosa.core.spectrum.griffinlim(abs_spectrogram)

print(audio_signal, audio_signal.shape)

# Podemos realizar la conversi√≥n inversa de un espectrograma a una imagen
librosa.output.write_wav('test2.wav', audio_signal, fs)
```

</div>
