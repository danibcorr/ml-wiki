---
sidebar_position: 3
authors:
  - name: Daniel Bazo Correa
description: Preguntas relacionadas con Deep Learning
title: Preguntas Deep Learning
toc_max_heading_level: 3
---

## Bibliografía

- [Deep Learning Interview Prep Course](https://youtu.be/BAregq0sdyY?si=xsq-901fJqlug4WY)

## Preguntas sobre Conceptos Básicos

<details>
<summary>
¿Qué es el aprendizaje profundo (deep learning)?
</summary>
Respuesta.
</details>

<details>
<summary>
¿En qué se diferencia el aprendizaje profundo de los modelos de aprendizaje automático (machine learning) tradicionales?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Qué es una red neuronal?
</summary>
Respuesta.
</details>

<details>
<summary>
Explica el concepto de neurona en el aprendizaje profundo.
</summary>
Respuesta.
</details>

<details>
<summary>
Explica la arquitectura de las redes neuronales de forma sencilla.
</summary>
Respuesta.
</details>

<details>
<summary>
¿Qué es una función de activación en las redes neuronales?
</summary>
Respuesta.
</details>

<details>
<summary>
Menciona algunas funciones de activación populares y descríbelas.
</summary>
Respuesta.
</details>

<details>
<summary>
¿Qué sucede si no utilizas ninguna función de activación en una red neuronal?
</summary>
Respuesta.
</details>

<details>
<summary>
Describe cómo funciona el entrenamiento de las redes neuronales básicas.
</summary>
Respuesta.
</details>

<details>
<summary>
¿Qué es el descenso de gradiente (gradient descent)?
</summary>
Respuesta.
</details>

## Preguntas sobre Optimización y Problemas de Gradiente

<details>
<summary>
¿Cuál es el papel del optimizador en el aprendizaje profundo?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Qué es la retropropagación (back propagation) y por qué es importante en el aprendizaje profundo?
</summary>
Respuesta.
</details>

<details>
<summary>
¿En qué se diferencia la retropropagación del descenso de gradiente?
</summary>
Respuesta.
</details>

<details>
<summary>
Describe qué es el problema del gradiente que se desvanece (vanishing gradient) y su impacto en las redes neuronales.
</summary>
Respuesta.
</details>

<details>
<summary>
¿Cuál es la conexión entre varias funciones de activación y el problema del gradiente que se desvanece?
</summary>
Respuesta.
</details>

<details>
<summary>
Hay una neurona en la capa oculta que siempre resulta en un gran error en la retropropagación, ¿cuál podría ser la razón?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Qué entiendes por un gráfico computacional?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Qué es la entropía cruzada (cross entropy) y por qué se prefiere como función de costo para problemas de clasificación multiclase?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Qué tipo de función de pérdida podemos aplicar cuando se trata de clasificación multiclase?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Qué es SGD (descenso de gradiente estocástico) y por qué se utiliza en el entrenamiento de redes neuronales?
</summary>
Respuesta.
</details>

## Preguntas sobre Variantes de Optimización y Tamaño de Lote

<details>
<summary>
¿Por qué el descenso de gradiente estocástico (SGD) oscila hacia el mínimo local?
</summary>
Respuesta.
</details>

<details>
<summary>
¿En qué se diferencia el GD del SGD?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Cómo podemos usar métodos de optimización como GD de una manera más mejorada? ¿Cuál es el papel del término de momento (momentum)?
</summary>
Respuesta.
</details>

<details>
<summary>
Compara el descenso de gradiente por lotes (batch gradient descent) con el descenso de gradiente por mini-lotes (mini batch gradient descent) y con el descenso de gradiente estocástico (stochastic gradient descent).
</summary>
Respuesta.
</details>

<details>
<summary>
¿Cómo decidir el tamaño del lote (batch size) en el aprendizaje profundo, considerando tamaños demasiado pequeños y demasiado grandes?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Cómo afecta el tamaño del lote al rendimiento de un modelo de aprendizaje profundo?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Qué es la matriz Hessiana (Haitian) y cómo se puede usar para un entrenamiento más rápido? ¿Cuáles son sus desventajas?
</summary>
Respuesta.
</details>

<details>
<summary>
Discute el concepto de tasa de aprendizaje adaptativa (adaptive learning rate). Describe los métodos de aprendizaje adaptativo.
</summary>
Respuesta.
</details>

<details>
<summary>
¿Qué es RMS Prop y cómo funciona?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Qué es Adam y por qué se usa la mayoría de las veces en redes neuronales?
</summary>
Respuesta.
</details>

## Preguntas sobre Normalización, Conexiones y Problemas de Gradiente

<details>
<summary>
¿Qué es Adam W y por qué se prefiere sobre Adam?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Qué es la normalización por lotes (batch normalization) y por qué se utiliza en redes neuronales?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Qué es la normalización por capa (layer normalization) y por qué se utiliza en redes neuronales?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Qué son las conexiones residuales (residual connections) y cuál es su función en las redes neuronales?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Qué es el recorte de gradiente (gradient clipping) y su impacto en la red neuronal?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Cuáles son las diferentes formas de resolver el problema del gradiente que se desvanece?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Cuáles son las formas de resolver los gradientes explosivos (exploding gradients)?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Qué sucede si la red neuronal sufre de sobreajuste (overfitting) y cómo se relaciona con los pesos en la red neuronal?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Qué es el abandono (dropout) y cómo funciona?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Cómo previene el abandono el sobreajuste en las redes neuronales?
</summary>
Respuesta.
</details>

## Preguntas sobre Regularización y Modelos Generativos

<details>
<summary>
¿Es el abandono como Random Forest?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Cuál es el impacto del abandono en el entrenamiento frente a las pruebas (testing)?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Qué son las regularizaciones L2 o L1 y cómo previenen el sobreajuste en las redes neuronales?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Cuál es la diferencia entre los enfoques de regularización L2 y L1?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Cómo impactan las regularizaciones L1 y L2 a los pesos en una red neuronal en lo que respecta a la comparación del impacto en penalizaciones de pesos grandes versus pequeños?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Qué es la maldición de la dimensionalidad (curse of dimensionality) en el aprendizaje automático o en la IA?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Cómo abordan los modelos de aprendizaje profundo la maldición de la dimensionalidad?
</summary>
Respuesta.
</details>

<details>
<summary>
¿Qué son los modelos generativos? Da ejemplos.
</summary>
Respuesta.
</details>
