---
sidebar_position: 9
authors:
  - name: Daniel Bazo Correa
description: Fundamentos del Deep Learning.
title: Neurociencia
toc_max_heading_level: 3
---

el cerebro crea mapas cognitivos que realiza una representación del mundo físico en un
mapa mental del que navega utiliza el hipocampo como memoria, navegación y planeamiento.
Las neuronas que se activan en posiciones determinadas, generan Speights o chispazos de
actividad, incluso en planos en tres dimensiones, lo que se conoce como place cell
activities. Esas neuronas al estimularlas normalmente afectan también en el hipocampo
existe un re mapeo de las neuronas porque el espacio cambia las neuronas no se activarán
en las mismas zonas al cambiar el entorno y depende la capacidad de re mapeo de la
similitud entre los entornos. cada entorno tiene un mapa interno. La neurona de
posicionamiento no solo obtiene información de su posición en el entorno sino que
identifica el entorno en sí pero no solo afectan cambios físicos del entorno factores
como los olores hacen que las celdas que las células se rematen, es decir, que son
importantes son dependientes del contexto no se lo almacena información espacial. El
hipocampo toma más variables para la tarea que son variables continuas el neocórtex pues
tiene en cuenta el lenguaje del conocimiento abstracto. Tenemos la columna cortical que
es la unidad fundamental de la inteligencia que se encuentra dentro del neocórtex.
Existe la hipótesis columnar que dice si el hardware que es la columna cortical
replicada es universal, igual para todas las réplicas que se realizan en el algoritmo
que permite su funcionamiento también lo debe ser. Esto se relaciona con el concepto de
la red de la inteligencia artificial general, la hagáis un único algoritmo universal
capaz de replicarse, adaptarse a cualquier entrada de manera universal donde solo cambia
el input la propia entrada sin embargo, cada columna cortical es un sistema
independiente y propio al final las columnas corticales son independientes, pero existe
un traspaso de mensajes entre columnas corticales, donde se traspasa de mensaje. Al
final puede verse como si fuese una API, incrementar el número de columnas corticales,
pues permite aumentar la capacidad de cómputo. Conforme obtenemos características del
mundo, pues vamos creando un Marco de referencia que permite localizar y relacionar
conceptos que posteriormente permiten generar nuevas recomendaciones de acciones que es
lo que se conoce como la integración de los caminos tenemos que la columna cortical está
dividido en seis niveles el nivel más alto es el nivel de reconexión con pocas neuronas,
el nivel dos y tres es el donde como en nivel cuatro que es la info obtiene información
de sensores como el ojo, el tacto, el nivel cinco, que es la acción y el nivel seis, que
es el donde se almacena información espacial oposición, el salto en el compartir
información de las capas de nivel seis a los niveles más altos de L2 L3, pues recuerda
mucho a las conexiones a las Skip Connections de la redes neuronales residuales lo que
ocurre en las redes convolucionales. Los Transformers funcionan de manera similar a las
máquinas de Tolman Eichenbaum TEM que obtienen información de diferentes entornos se
realiza una auto correlación para ver la relación entre los entornos que sería el
mecanismo de atención y se estudia la acción o posición tomada. Tenemos que las columnas
corticales hacen lo que se conoce como el Mayor botín que es la votación a mayores que
llegan a un consenso de lo que percibe a la entrada y la entrada de una columna cortical
pues puede ser también información de otra columna cortical.

Al final tenemos una neurona donde la entrada o la parte inicial de la neurona son las
de antes son las dendritas que es donde se introduce la entrada tenemos el soma que es
donde se realiza el procesamiento y tenemos el Acson que es la salida. Las neuronas
activan cuando el nivel de las impulsos de las entradas de las dendritas alcanzan un
umbral que se conoce como el Spike neutrón llegando al soma y creando una proyección del
resultado por el Acson. Cuando la información del Acson se traspasa otra neurona a
través de la sinapsis el traspaso de información pasa de un proceso eléctrico a ser un
proceso químico, lo que se conoce como neurotransmisores. En la neurona siguiente, con
la llegada de los neurotransmisores se crea una nueva señal eléctrica. La sinapsis puede
ampliarse gracias a la plasticidad permitiendo modificar la conexión con otras neuronas
la sinapsis cuenta con un detector de coincidencias que cuando se está lo
suficientemente polarizado realmente despolariza. qué ocurre gracias a un receptor de N
Medea que te detecta coincidencias en una señal química. Llegada de neurotransmisores de
otra neurona y una señal eléctrica que es cuando la neurona receptora se activa y su
membrana se les se despolariza siendo más Positivo. Esta despolarización ocurre por el
mecanismo hebbiano donde las neuronas que se activan juntas se conectan juntas y por
mecanismos locales, que son varias sinapsis cercanas activas. A la vez esto permite
aumentar o disminuir el número de receptores que es la plasticidad sináptica. Una
neurona cuenta con dos partes diferenciadas según la conexión tenemos las conexiones
mediante feedback, que son información asociativa de áreas superiores que son el apical
las dendritas apicales y tenemos las feedforward connections, que es computación local
con información, sensorial y motora, que son las dendritas basales. Al parecer no hay
una cantidad máxima de neuro transmisión, es que puede existir en una sinapsis la
plasticidad que está muy relacionada con la información de las vecinas. Las dendritas
apicales pueden modificarse según los patrones de entrada en su vecindario. Las
dendritas basales depende más en la contribución de la salida de las neuronas, por
tanto, la redes neuronales artificiales modelan la integración basal local, más
plasticidad, dependiente del error de salida, pero no modelan la señales apicales, ni la
plasticidad dependiente del contexto global o vecindarios neuronales.

